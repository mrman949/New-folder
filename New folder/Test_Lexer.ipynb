{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    These tests are complete.\\n    Feel free to point out any errors or tests that should be present but are not.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    These tests are complete.\n",
    "    Feel free to point out any errors or tests that should be present but are not.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \"from Lexemes.Lexemes import *\" points to Lexemes.py in the Lexemes subfolder\n",
    "from Lexer.Lexemes.Lexemes import * # should point to your Lexemes.py file\n",
    "from Lexer.Token import Token\n",
    "from Lexer.Lexer import Lexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a new Lexer\n",
    "lexer = Lexer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lexer(lexer:Lexer, test:list) -> bool:\n",
    "    char_stream = test[0]\n",
    "    expected_tokens = test[1]\n",
    "\n",
    "    matched_tokens = lexer.match_tokens(char_stream)\n",
    "\n",
    "    i = j = 0\n",
    "    all_correct = True\n",
    "\n",
    "    while i < len(expected_tokens) and j < len(matched_tokens):\n",
    "        if expected_tokens[i] == matched_tokens[j]:\n",
    "            pass\n",
    "            # print(\"matched: expected:{} and got:{}\".format(expected_tokens[i], matched_tokens[j]))\n",
    "        else:\n",
    "            all_correct = False\n",
    "            print(\"NO: expected:{} and got:{}\".format(expected_tokens[i], matched_tokens[j]))\n",
    "        i += 1\n",
    "        j += 1\n",
    "\n",
    "    while i < len(expected_tokens):\n",
    "        all_correct = False\n",
    "        print(\"NO: expected:{} and got:{}\".format(expected_tokens[i], None))\n",
    "        i += 1\n",
    "    \n",
    "    while j < len(matched_tokens):\n",
    "        all_correct = False\n",
    "        print(\"NO: expected:{} and got:{}\".format(None, matched_tokens[i]))\n",
    "        j += 1\n",
    "\n",
    "    return all_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code written in class:\n",
    "\n",
    "# def test_lexer(lexer:Lexer, test:list) -> bool:\n",
    "#     char_stream = test[0]\n",
    "#     expected_tokens = test[1]\n",
    "\n",
    "#     matched_tokens = lexer.match_tokens(char_stream)\n",
    "\n",
    "#     i = j = 0\n",
    "#     all_correct = True\n",
    "\n",
    "#     while i < len(expected_tokens) and j < len(matched_tokens):\n",
    "#         if expected_tokens[i] == matched_tokens[j]:\n",
    "#             pass\n",
    "#         else:\n",
    "#             print(\"NOT matched: expected {}, got {}\".format(expected_tokens[i], matched_tokens[j]))\n",
    "#             all_correct = False\n",
    "#         i += 1\n",
    "#         j += 1\n",
    "\n",
    "#     while i < len(expected_tokens):\n",
    "#         print(\"(expected token) No possible match for {}\".format(expected_tokens[i]))\n",
    "#         i += 1\n",
    "#         all_correct = False\n",
    "    \n",
    "#     while j < len(matched_tokens):\n",
    "#         print(\"(matched token) No possible match for {}\".format(matched_tokens[i]))\n",
    "#         j += 1\n",
    "#         all_correct = False\n",
    "\n",
    "#     return all_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_stream_1 = \"\"\"\n",
    "x = -9;\n",
    "y = 3.0;\n",
    "print(True + x * y);\n",
    "\"\"\"\n",
    "\n",
    "expected_tokens_1 = [\n",
    "    Token(Token_Enum.Identifiers.Iden,\"x\"),\n",
    "    Token(Token_Enum.Operators.Equal,\"=\"),\n",
    "    Token(Token_Enum.Literals.Int,\"-9\"),\n",
    "    Token(Token_Enum.Line_End.Line_End,\";\"),\n",
    "\n",
    "    Token(Token_Enum.Identifiers.Iden,\"y\"),\n",
    "    Token(Token_Enum.Operators.Equal,\"=\"),\n",
    "    Token(Token_Enum.Literals.Float,\"3.0\"),\n",
    "    Token(Token_Enum.Line_End.Line_End,\";\"),\n",
    "\n",
    "    Token(Token_Enum.Functions.Print,\"print\"),\n",
    "    Token(Token_Enum.Closures.Paren_Open,\"(\"),\n",
    "    Token(Token_Enum.Literals.Bool,\"True\"),\n",
    "    Token(Token_Enum.Operators.Add,\"+\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"x\"),\n",
    "    Token(Token_Enum.Operators.Multiply,\"*\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"y\"),\n",
    "    Token(Token_Enum.Closures.Paren_Close,\")\"),\n",
    "    Token(Token_Enum.Line_End.Line_End,\";\"),\n",
    "]\n",
    "\n",
    "test_1 = (char_stream_1, expected_tokens_1)\n",
    "\n",
    "test_lexer(lexer, test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_stream_2 = \"\"\"\n",
    "i = 42;\n",
    "j = 35;\n",
    "while (i != j){\n",
    "    if (i > j){\n",
    "        i = i - j;\n",
    "    } else {\n",
    "        j = j - i;\n",
    "    };\n",
    "};\n",
    "print(\"i is equal to\");\n",
    "print(i);\n",
    "\"\"\"\n",
    "\n",
    "expected_tokens_2 = [\n",
    "    # line 2: i = 42;\n",
    "    Token(Token_Enum.Identifiers.Iden,\"i\"),\n",
    "    Token(Token_Enum.Operators.Equal,\"=\"),\n",
    "    Token(Token_Enum.Literals.Int,\"42\"),\n",
    "    Token(Token_Enum.Line_End.Line_End,\";\"),\n",
    "\n",
    "    # line 3: j = 35;\n",
    "    Token(Token_Enum.Identifiers.Iden,\"j\"),\n",
    "    Token(Token_Enum.Operators.Equal,\"=\"),\n",
    "    Token(Token_Enum.Literals.Int,\"35\"),\n",
    "    Token(Token_Enum.Line_End.Line_End,\";\"),\n",
    "\n",
    "    # line 4: while (i != j){\n",
    "    Token(Token_Enum.Keywords.While,\"while\"),\n",
    "    Token(Token_Enum.Closures.Paren_Open,\"(\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"i\"),\n",
    "    Token(Token_Enum.Operators.Not_Equal,\"!=\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"j\"),\n",
    "    Token(Token_Enum.Closures.Paren_Close,\")\"),\n",
    "    Token(Token_Enum.Closures.Curly_Open,\"{\"),\n",
    "\n",
    "    # line 5:       if (i > j){\n",
    "    Token(Token_Enum.Keywords.If,\"if\"),\n",
    "    Token(Token_Enum.Closures.Paren_Open,\"(\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"i\"),\n",
    "    Token(Token_Enum.Operators.Greater,\">\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"j\"),\n",
    "    Token(Token_Enum.Closures.Paren_Close,\")\"),\n",
    "    Token(Token_Enum.Closures.Curly_Open,\"{\"),  \n",
    "\n",
    "    # line 6:           i = i - j;\n",
    "    Token(Token_Enum.Identifiers.Iden,\"i\"),\n",
    "    Token(Token_Enum.Operators.Equal,\"=\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"i\"),\n",
    "    Token(Token_Enum.Operators.Subtract,\"-\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"j\"),\n",
    "    Token(Token_Enum.Line_End.Line_End,\";\"),\n",
    "\n",
    "    # line 7:       } else {\n",
    "    Token(Token_Enum.Closures.Curly_Close,\"}\"),\n",
    "    Token(Token_Enum.Keywords.Else,\"else\"),\n",
    "    Token(Token_Enum.Closures.Curly_Open,\"{\"),  \n",
    "    \n",
    "    # line 8:           j = j - i;\n",
    "    Token(Token_Enum.Identifiers.Iden,\"j\"),\n",
    "    Token(Token_Enum.Operators.Equal,\"=\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"j\"),\n",
    "    Token(Token_Enum.Operators.Subtract,\"-\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"i\"),\n",
    "    Token(Token_Enum.Line_End.Line_End,\";\"),\n",
    "\n",
    "    # line 9:       };\n",
    "    Token(Token_Enum.Closures.Curly_Close,\"}\"),\n",
    "    Token(Token_Enum.Line_End.Line_End,\";\"),\n",
    "\n",
    "    # line 10: };\n",
    "    Token(Token_Enum.Closures.Curly_Close,\"}\"),\n",
    "    Token(Token_Enum.Line_End.Line_End,\";\"),\n",
    "\n",
    "    # line 11: print(\"i is equal to\");\n",
    "    Token(Token_Enum.Functions.Print,\"print\"),\n",
    "    Token(Token_Enum.Closures.Paren_Open,\"(\"),\n",
    "    Token(Token_Enum.Literals.String,'i is equal to'),\n",
    "    Token(Token_Enum.Closures.Paren_Close,\")\"),\n",
    "    Token(Token_Enum.Line_End.Line_End,\";\"),\n",
    "\n",
    "    # line 12: print(i);\n",
    "    Token(Token_Enum.Functions.Print,\"print\"),\n",
    "    Token(Token_Enum.Closures.Paren_Open,\"(\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"i\"),\n",
    "    Token(Token_Enum.Closures.Paren_Close,\")\"),\n",
    "    Token(Token_Enum.Line_End.Line_End,\";\"),\n",
    "]\n",
    "\n",
    "test_2 = (char_stream_2, expected_tokens_2)\n",
    "\n",
    "test_lexer(lexer, test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_stream_3 = \"\"\"\n",
    "string_1 = \"This is\n",
    "all one big\n",
    "string\"\n",
    "\"\"\"\n",
    "\n",
    "expected_tokens_3 = [\n",
    "    Token(Token_Enum.Identifiers.Iden,\"string_1\"),\n",
    "    Token(Token_Enum.Operators.Equal,\"=\"),\n",
    "    Token(Token_Enum.Literals.String,'This is\\nall one big\\nstring'),\n",
    "]\n",
    "\n",
    "test_3 = (char_stream_3, expected_tokens_3)\n",
    "\n",
    "test_lexer(lexer, test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_stream_4 = \"\"\"\n",
    "log _log log_ log3 3log\n",
    "\"\"\"\n",
    "\n",
    "expected_tokens_4 = [\n",
    "    Token(Token_Enum.Functions.Log,\"log\"),\n",
    "\n",
    "    Token(Token_Enum.Identifiers.Iden,\"_log\"),\n",
    "\n",
    "    Token(Token_Enum.Identifiers.Iden,\"log_\"),\n",
    "\n",
    "    Token(Token_Enum.Identifiers.Iden,\"log3\"),\n",
    "\n",
    "    Token(Token_Enum.Literals.Int,\"3\"),\n",
    "    Token(Token_Enum.Functions.Log,\"log\"),\n",
    "]\n",
    "\n",
    "test_4 = (char_stream_4, expected_tokens_4)\n",
    "\n",
    "test_lexer(lexer, test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lxm_Keywords_If,Lxm_Keywords_Else,Lxm_Keywords_While,\n",
    "# Lxm_Function_Log,Lxm_Function_Print,Lxm_Function_To_Int,Lxm_Function_To_Float,Lxm_Function_To_Bool,Lxm_Function_To_String,\n",
    "\n",
    "char_stream_5 = \"\"\"\n",
    "if iff\n",
    "else elsee\n",
    "while whilee\n",
    "log logg\n",
    "print printt\n",
    "to_int to_intt\n",
    "to_float to_floatt\n",
    "to_bool to_booll\n",
    "to_string to_stringg\n",
    "True Truee\n",
    "False Falsee\n",
    "\"\"\"\n",
    "\n",
    "expected_tokens_5 = [\n",
    "    Token(Token_Enum.Keywords.If,\"if\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"iff\"),\n",
    "\n",
    "    Token(Token_Enum.Keywords.Else,\"else\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"elsee\"),\n",
    "\n",
    "    Token(Token_Enum.Keywords.While,\"while\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"whilee\"),\n",
    "\n",
    "    Token(Token_Enum.Functions.Log,\"log\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"logg\"),\n",
    "\n",
    "    Token(Token_Enum.Functions.Print,\"print\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"printt\"),\n",
    "\n",
    "    Token(Token_Enum.Functions.To_Int,\"to_int\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"to_intt\"),\n",
    "\n",
    "    Token(Token_Enum.Functions.To_Float,\"to_float\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"to_floatt\"),\n",
    "\n",
    "    Token(Token_Enum.Functions.To_Bool,\"to_bool\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"to_booll\"),\n",
    "\n",
    "    Token(Token_Enum.Functions.To_String,\"to_string\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"to_stringg\"),\n",
    "\n",
    "    Token(Token_Enum.Literals.Bool,\"True\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"Truee\"),\n",
    "\n",
    "    Token(Token_Enum.Literals.Bool,\"False\"),\n",
    "    Token(Token_Enum.Identifiers.Iden,\"Falsee\"),\n",
    "]\n",
    "\n",
    "test_5 = (char_stream_5, expected_tokens_5)\n",
    "\n",
    "test_lexer(lexer, test_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lxm_Keywords_If,Lxm_Keywords_Else,Lxm_Keywords_While,\n",
    "# Lxm_Function_Log,Lxm_Function_Print,Lxm_Function_To_Int,Lxm_Function_To_Float,Lxm_Function_To_Bool,Lxm_Function_To_String,\n",
    "\n",
    "char_stream_6 = \"\"\"\n",
    "-\n",
    "\n",
    "1.0\n",
    "-1.0\n",
    ".0\n",
    "1.\n",
    "\n",
    "1\n",
    "-1\n",
    "10\n",
    "-10\n",
    "\"\"\"\n",
    "\n",
    "expected_tokens_6 = [\n",
    "    Token(Token_Enum.Operators.Subtract,\"-\"),\n",
    "\n",
    "    Token(Token_Enum.Literals.Float,\"1.0\"),\n",
    "    Token(Token_Enum.Literals.Float,\"-1.0\"),\n",
    "    Token(Token_Enum.Literals.Float,\".0\"),\n",
    "    Token(Token_Enum.Literals.Float,\"1.\"),\n",
    "\n",
    "    Token(Token_Enum.Literals.Int,\"1\"),\n",
    "    Token(Token_Enum.Literals.Int,\"-1\"),\n",
    "    Token(Token_Enum.Literals.Int,\"10\"),\n",
    "    Token(Token_Enum.Literals.Int,\"-10\"),\n",
    "]\n",
    "\n",
    "test_6 = (char_stream_6, expected_tokens_6)\n",
    "\n",
    "test_lexer(lexer, test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
